# SimiliAI â€” Neural Image Similarity Search

<div align="center">

```
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—      â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
 â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘      â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘      â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
 â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘      â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
 â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
```

**AI-Powered Image Similarity Search using Triplet Networks**

![Python](https://img.shields.io/badge/Python-3.12-blue?style=flat-square&logo=python)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange?style=flat-square&logo=tensorflow)
![Streamlit](https://img.shields.io/badge/Streamlit-UI-red?style=flat-square&logo=streamlit)
![CIFAR-10](https://img.shields.io/badge/Dataset-CIFAR--10-green?style=flat-square)
![License](https://img.shields.io/badge/License-MIT-yellow?style=flat-square)

*Find visually similar images in milliseconds using deep learned embeddings â€” trained entirely from scratch.*

</div>

---

## ğŸ“– Table of Contents

- [Project Overview](#-project-overview)
- [Problem Statement](#-problem-statement)
- [How It Works](#-how-it-works)
- [Architecture](#-architecture)
- [Triplet Loss](#-triplet-loss)
- [Project Structure](#-project-structure)
- [Dataset](#-dataset)
- [Setup & Installation](#-setup--installation)
- [Running the Pipeline](#-running-the-pipeline)
- [Streamlit Application](#-streamlit-application)
- [Results](#-results)
- [Scripts Reference](#-scripts-reference)
- [Technical Details](#-technical-details)
- [Limitations & Future Work](#-limitations--future-work)

---

## ğŸ§  Project Overview

**SimiliAI** is a deep learning system that learns to identify and retrieve visually similar images from a large database â€” without using any text labels, manual tags, or pre-trained models. Everything is **trained from scratch**.

Given a query image, SimiliAI:
1. Encodes it into a compact **128-dimensional embedding vector**
2. Searches a pre-built database of embeddings using **Euclidean distance**
3. Returns the **top-K most visually similar images** in real time

This mirrors real-world systems like Pinterest's visual search, Getty Images' visual discovery, e-commerce "shop the look" features, and Google Lens.

---

## â“ Problem Statement

In today's digital world, we are surrounded by millions of images across social media, e-commerce, photo galleries, and digital libraries. Users often struggle to find visually similar images or get relevant recommendations based on image content.

**Traditional text-based search fails** to capture visual semantics â€” a photo of a golden retriever and a labrador look identical to a human but have completely different tags.

**Challenge:** How can we automatically identify and recommend visually similar images from a large database without relying on manual tags or text descriptions?

**Real-world applications:**
- ğŸ›’ **E-commerce** â€” "Find similar products" on shopping websites
- ğŸ“¸ **Social media** â€” Instagram's "Related Posts" recommendations
- ğŸ–¼ï¸ **Stock photography** â€” Getty Images' visual search
- ğŸ‘— **Fashion** â€” "Shop the Look" features

---

## âš™ï¸ How It Works

SimiliAI uses a **Triplet Network** â€” a Siamese-style architecture that learns a metric embedding space where:

```
distance(same class images)     â†’  SMALL
distance(different class images) â†’  LARGE
```

### Training Phase
The model trains on **triplets** of images:

```
Anchor  (A) â”€â”€â”
Positive (P) â”€â”¼â”€â”€â–º CNN â”€â”€â–º Embeddings â”€â”€â–º Triplet Loss
Negative (N) â”€â”˜
```

- **Anchor** â€” a reference image
- **Positive** â€” a different image of the **same class** as anchor
- **Negative** â€” an image of a **different class** from anchor

The loss function teaches the network: *"keep anchor close to positive, push anchor away from negative."*

### Search Phase
At query time:
1. Query image â†’ CNN â†’ 128-dim embedding vector
2. Compute Euclidean distance from query to all database embeddings
3. Sort by distance (smallest = most similar)
4. Return top-K results

---

## ğŸ—ï¸ Architecture

### Base CNN (Embedding Model)

```
Input (64Ã—64Ã—3)
       â”‚
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
  â”‚ Conv2D  â”‚  32 filters, 3Ã—3, ReLU, Same padding
  â”‚ BatchNormâ”‚
  â”‚ MaxPool â”‚  2Ã—2
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â”‚
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
  â”‚ Conv2D  â”‚  64 filters, 3Ã—3, ReLU, Same padding
  â”‚ BatchNormâ”‚
  â”‚ MaxPool â”‚  2Ã—2
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â”‚
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
  â”‚ Conv2D  â”‚  128 filters, 3Ã—3, ReLU, Same padding
  â”‚ BatchNormâ”‚
  â”‚ GlobalAvgPoolâ”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â”‚
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
  â”‚ Dense   â”‚  256 units, ReLU
  â”‚ Dropout â”‚  0.3
  â”‚ Dense   â”‚  128 units, tanh
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â”‚
  128-dim Embedding âˆˆ [-1, 1]
```

**Total parameters:** ~2.2M (8.49 MB)

### Training Model (Triplet Wrapper)

The training model wraps the base CNN three times (shared weights):

```
Anchor  â”€â”€â–º [Base CNN] â”€â”€â”
Positive â”€â”€â–º [Base CNN] â”€â”€â”¼â”€â”€â–º Concatenate â”€â”€â–º Triplet Loss
Negative â”€â”€â–º [Base CNN] â”€â”€â”˜
```

After training, only the **Base CNN** is saved and used for inference.

### Key Design Choices

| Choice | Reason |
|--------|--------|
| `tanh` output activation | Bounds embeddings to [-1, 1], prevents embedding collapse |
| `GlobalAveragePooling2D` | Better than Flatten for small datasets â€” reduces overfitting |
| `BatchNormalization` | Stabilises training, allows higher learning rates |
| `Dropout(0.3)` | Regularisation for 2000-image dataset |
| Euclidean distance search | Matches the triplet loss metric â€” correct at inference time |

---

## ğŸ“ Triplet Loss

The core learning signal:

```
L(A, P, N) = max( d(A, P) - d(A, N) + Î±, 0 )
```

Where:
- `d(x, y) = âˆšÎ£(xáµ¢ - yáµ¢)Â²` â€” Euclidean distance
- `A` â€” Anchor embedding
- `P` â€” Positive embedding (same class)
- `N` â€” Negative embedding (different class)
- `Î± = 0.3` â€” Margin (minimum separation required)

**Intuition:**
- If `d(A,P) + Î± < d(A,N)` â€” loss = 0 (already well separated âœ“)
- If `d(A,N) - d(A,P) < Î±` â€” positive loss (needs more separation âœ—)

### Triplet Sampling Strategy

SimiliAI uses **pre-built random triplet sampling**:
- 8,000 training triplets built upfront in RAM (pure NumPy)
- 1,000 validation triplets
- Passed to `model.fit()` in batches of 32
- No per-step model calls during data generation â†’ **10Ã— faster** than online mining

This approach is optimal for CPU training on small datasets.

---
```
## Folder Structure

AI_Image_Similarity_Search/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/
â”‚ â””â”€â”€ processed/
â”œâ”€â”€ models/
â”‚ â””â”€â”€ embedding_model.keras
â”œâ”€â”€ results/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ train_triplet.py
â”‚ â”œâ”€â”€ prepare_dataset.py
â”‚ â””â”€â”€ search.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

---
```
## How To Run

1. Train the model:

python src/train_triplet.py


2. Prepare embeddings:

python src/prepare_dataset.py


3. Run similarity search:

python src/search.py


Results will be saved in:

results/


---

## Current Status

- Working similarity search system
- 2000 images indexed
- Top-5 accuracy ~36%
- Embedding size: 128

---

## ğŸ“œ Scripts Reference

| Script | Purpose | Inputs | Outputs |
|--------|---------|--------|---------|
| `download_dataset.py` | Download CIFAR-10, save as PNG with sharpening | â€” | `data/raw/cifar10_images/` |
| `split_dataset.py` | 80/20 train/test split | `cifar10_images/` | `data/raw/train/`, `data/raw/test/` |
| `train_triplet.py` | Train CNN with triplet loss | `data/raw/train/` | `models/embedding_model.keras`, `results/training_curves.png`, `logs/training_log.csv` |
| `prepare_dataset.py` | Extract 128-dim embeddings | `models/embedding_model.keras` | `data/processed/*.npy` (6 files) |
| `search.py` | Benchmark k-NN search evaluation | `data/processed/*.npy` | `results/benchmark_*.png`, console report |
| `app.py` | Streamlit web application | `data/processed/*.npy`, `models/embedding_model.keras` | Interactive UI at localhost:8501 |

---

## ğŸ”¬ Technical Details

### Why Euclidean Distance (not Cosine Similarity)?

The triplet loss is defined as:
```
L = max( d(A,P) - d(A,N) + Î±, 0 )
```
where `d` is **Euclidean distance**. This means the embedding space is geometrically shaped by Euclidean relationships during training. Using cosine similarity at search time would measure angles instead of distances â€” ignoring the actual structure learned by the loss function and producing incorrect rankings.

### Why `tanh` on the Output Layer?

Without output normalisation, CNN embeddings can collapse â€” all vectors pointing in nearly the same direction, making cosine similarity always â‰ˆ 1.0 and Euclidean distances meaningless. `tanh` bounds each embedding dimension to `[-1, 1]`, which:
- Prevents embedding collapse
- Keeps distances in a meaningful range
- Does not enforce equal magnitude (unlike L2 normalisation, which caused degenerate results)

### Why GlobalAveragePooling2D (not Flatten)?

On small datasets like 2,000 images, `Flatten` produces a huge feature vector (8,192-dim) that leads to severe overfitting. `GlobalAveragePooling2D` averages each feature map to a single value, producing a 128-dim vector that captures spatial patterns without memorising positions â€” much better generalisation.

### Why Random Triplets (not Semi-Hard Mining)?

Semi-hard mining calls the model during data generation for every single triplet, which on CPU means 10â€“40 minutes per epoch. By pre-building 8,000 triplets in RAM using pure NumPy before training begins and using `model.fit()` for batching, each epoch completes in 2â€“5 minutes â€” a 10Ã— speedup with comparable accuracy on this dataset size.

---

## Conclusion

This is a working AI-based image similarity search engine using deep learning and Triplet Loss.
